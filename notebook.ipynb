{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZizZu94/nlu-second-assignment/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff_g7KO4jncR"
      },
      "source": [
        "# **[NLU] Second Assignment** \n",
        "*   **Zihadul Azam**\n",
        "*   Id: 221747\n",
        "*   zihadul.azam@studenti.unitn.it\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE0k7yO7jVyu"
      },
      "source": [
        "Commands to set Colab environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viBTfFZihwGR",
        "outputId": "89afb9a7-e697-4626-f32b-05a19a37c556"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiyDkSqaieap"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/My Drive/Colab Notebooks/nlu/second-assignment/nlu-second-assignment')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-nQ-DDtk3jH"
      },
      "source": [
        "!cd '/content/drive/My Drive/Colab Notebooks/nlu/second-assignment/nlu-second-assignment'"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EH4Sos_lSm1",
        "outputId": "68998657-ea63-4386-f5b3-4ba31ee163b4"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4CIIOKwiP2c"
      },
      "source": [
        "### **Requirements**\n",
        "\n",
        "\n",
        "*   SpaCy: run `pip install spacy`\n",
        "*   Sk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IstODjTEji7U"
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "import conll\n",
        "\n",
        "from nltk.metrics import *\n",
        "\n",
        "# utils\n",
        "import utils"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rcUm2ZSg-BQ"
      },
      "source": [
        "Load nlp and set white space tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3WMxOpWg1xt"
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "\n",
        "class WhitespaceTokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __call__(self, text):\n",
        "        words = text.split(\" \")\n",
        "        return Doc(self.vocab, words=words)\n",
        "\n",
        "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZNmI2DAjz6H"
      },
      "source": [
        "#### **Global vars**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IVP_lysj4zW"
      },
      "source": [
        "data_folder_path = './data'\n",
        "train_path = data_folder_path + '/train.txt'\n",
        "test_path = data_folder_path + '/test.txt'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5Okb0VMkBOF"
      },
      "source": [
        "#### **Load input data**\n",
        "Load conll 2003 data from files and pre-process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7vnghqXkKbJ"
      },
      "source": [
        "def load_conll_file(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "    sentences = data.split('\\n\\n')\n",
        "    OUTPUT_DATA = []\n",
        "    entities = []\n",
        "    for sent in sentences:\n",
        "        tokens = sent.split('\\n')\n",
        "        sentence = []\n",
        "        ent_sentence_spacy = []\n",
        "        ents = []\n",
        "\n",
        "        if tokens[0] != '-DOCSTART- -X- -X- O' and tokens[0] != '':\n",
        "            for x in tokens:\n",
        "                x_split = x.split()\n",
        "                # if not short length\n",
        "                if len(x) > 0 and len(x_split) >= 3:\n",
        "                    word = x_split[0]\n",
        "                    word = word.strip()\n",
        "\n",
        "                    if len(word) > 0:\n",
        "                        sentence.append(word)\n",
        "                        try:\n",
        "                            ent = x_split[-1]\n",
        "                        except IndexError:\n",
        "                            print('Index Error: ', x_split)\n",
        "                        ents.append((word, ent))\n",
        "                # else:\n",
        "                    #print('Short length x: ', x, ' . Removed.')\n",
        "\n",
        "            processed_sentence = ' '.join(sentence)  # .lower()\n",
        "            OUTPUT_DATA.append((processed_sentence, {'entities': ents}))\n",
        "\n",
        "    print('Done getting data !')\n",
        "    print('There are %d sentences.' % (len(sentences)))\n",
        "    return OUTPUT_DATA"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJlxoMSSkYMr"
      },
      "source": [
        "Mapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9edtYHD2kdVX"
      },
      "source": [
        "def convert_spacy_entity_to_conll(iob, type):\n",
        "    mapper = {\n",
        "        'PERSON': 'PER',\n",
        "        'NORP': 'MISC',\n",
        "        'FAC': 'MISC',\n",
        "        'ORG': 'ORG',\n",
        "        'GPE': 'LOC',\n",
        "        'LOC': 'LOC',\n",
        "        'PRODUCT': 'MISC',\n",
        "        'EVENT': 'MISC',\n",
        "        'WORK_OF_ART': 'MISC',\n",
        "        'LAW': 'MISC',\n",
        "        'LANGUAGE': 'MISC',\n",
        "        'DATE': 'MISC',\n",
        "        'TIME': 'MISC',\n",
        "        'PERCENT': 'MISC',\n",
        "        'MONEY': 'MISC',\n",
        "        'QUANTITY': 'MISC',\n",
        "        'ORDINAL': 'MISC',\n",
        "        'CARDINAL':  'MISC'\n",
        "    }\n",
        "\n",
        "    if not type in mapper:\n",
        "        return 'O'\n",
        "    return iob + '-' + mapper[type]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfDx-Kybke1O"
      },
      "source": [
        "Get ref and hyp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXEpJrEckkkz"
      },
      "source": [
        "def get_refs_hyps(data):\n",
        "    index = 0\n",
        "    total_data = len(data)\n",
        "\n",
        "    refs = []\n",
        "    hyps = []\n",
        "\n",
        "    for sent in data:\n",
        "        entities = sent[1]['entities']\n",
        "        hyp = []\n",
        "        doc = nlp(sent[0])\n",
        "        for token in doc:\n",
        "            conll_entity = convert_spacy_entity_to_conll(\n",
        "                token.ent_iob_, token.ent_type_)\n",
        "\n",
        "            hyp.append((token.text, conll_entity))\n",
        "        index += 1\n",
        "\n",
        "        # add this sentence refs\n",
        "        sent_refs = [(entity[0], entity[1]) for entity in entities]\n",
        "        refs.append(sent_refs)\n",
        "        # add this sent hyp\n",
        "        hyps.append(hyp)\n",
        "\n",
        "        utils.printProgressBar(index, total_data,\n",
        "                               prefix='Progress calculating accuracy:', suffix='Complete', length=50)\n",
        "    return refs, hyps"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC0D8MqZkoDL"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "yvdfYovukrY8",
        "outputId": "d3e4ae03-b99d-429c-f7d8-8379f2aee457"
      },
      "source": [
        "conll_test_data = load_conll_file(test_path)\n",
        "print('------------ First 2 sentences ------------')\n",
        "for k in conll_test_data[:2]:\n",
        "    print('Sentence: ', k[0])\n",
        "    print('Entities:')\n",
        "    for e in k[1]['entities']:\n",
        "        print(e[0], '|', e[1])\n",
        "\n",
        "    print('************************************')\n",
        "\n",
        "print('\\n')\n",
        "print('------------ Task 1.1: report token-level performance (per class and total) ------------')\n",
        "refs, hyps = get_refs_hyps(conll_test_data)\n",
        "accuracy = conll.evaluate(refs, hyps)\n",
        "print(accuracy)\n",
        "\n",
        "entity_refs = []\n",
        "for sent in refs:\n",
        "    for token in sent:\n",
        "        entity_refs.append(token[1])\n",
        "\n",
        "entity_hyps = []\n",
        "for sent in hyps:\n",
        "    for token in sent:\n",
        "        entity_hyps.append(token[1])\n",
        "\n",
        "confusion_matrix = ConfusionMatrix(entity_refs, entity_hyps)\n",
        "print(confusion_matrix)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-1df8149a9271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconll_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_conll_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------------ First 2 sentences ------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconll_test_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentence: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Entities:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-5c6514a6d34a>\u001b[0m in \u001b[0;36mload_conll_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_conll_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mOUTPUT_DATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/test.txt'"
          ]
        }
      ]
    }
  ]
}